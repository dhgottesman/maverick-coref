{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41bc2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JUPYTER_CONFIG_DIR\"] = \"/home/morg/students/gottesman3/.jupyter\"\n",
    "os.environ[\"JUPYTER_DATA_DIR\"] = \"/home/morg/students/gottesman3/.local/share/jupyter\"\n",
    "os.environ[\"JUPYTER_RUNTIME_DIR\"] = \"/home/morg/students/gottesman3/.local/share/jupyter/runtime\"\n",
    "\n",
    "# Then import widgets etc.\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756bd067",
   "metadata": {},
   "source": [
    "# Refined LMBD \n",
    "## For retrieving metadata about QIDs such as title and url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a86db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Any, Dict, Mapping, Iterable\n",
    "from typing import TypeVar\n",
    "\n",
    "import lmdb\n",
    "import ujson as json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "K = TypeVar('K')\n",
    "V = TypeVar('V')\n",
    "\n",
    "\n",
    "def batch_items(iterable: Iterable[Any], n: int = 1):\n",
    "    \"\"\"\n",
    "    Batches an iterables by yielding lists of length n. Final batch length may be less than n.\n",
    "    :param iterable: any iterable\n",
    "    :param n: batch size (final batch may be truncated)\n",
    "    \"\"\"\n",
    "    current_batch = []\n",
    "    for item in iterable:\n",
    "        current_batch.append(item)\n",
    "        if len(current_batch) == n:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "\n",
    "class LmdbImmutableDict(Mapping[K, V]):\n",
    "    def __iter__(self):\n",
    "        NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, key: K) -> V:\n",
    "        if not key:\n",
    "            raise KeyError(key)\n",
    "        with self.env.begin() as txn:\n",
    "            value = txn.get(self.encode(key))\n",
    "        if value is None:\n",
    "            raise KeyError(key)\n",
    "        return self.decode(value)\n",
    "\n",
    "    def encode(self, key: K) -> bytes:\n",
    "        try:\n",
    "            return key.encode(\"utf-8\")\n",
    "        except UnicodeEncodeError as err:\n",
    "            warnings.warn(f'Unable to encode key {key}, err: {err}')\n",
    "\n",
    "    def decode(self, value: bytes) -> Dict[Any, Any]:\n",
    "        return json.loads(value.decode(\"utf-8\"))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        with self.env.begin() as txn:\n",
    "            return txn.stat()[\"entries\"]\n",
    "\n",
    "    def __init__(self, path: str, write_mode: bool = False):\n",
    "        if write_mode:\n",
    "            self.path = f'{path}.incomplete'\n",
    "            self.env = lmdb.open(self.path, max_dbs=1, readonly=False, create=True, writemap=True,\n",
    "                                 subdir=False, map_size=1099511627776 * 2,\n",
    "                                 meminit=False, map_async=True, mode=0o755,\n",
    "                                 lock=False)\n",
    "        else:\n",
    "            self.path = path\n",
    "            self.env = lmdb.open(self.path, max_dbs=1, readonly=True, create=True, writemap=False,\n",
    "                                 subdir=False, map_size=1099511627776 * 2,\n",
    "                                 meminit=False, map_async=True, mode=0o755,\n",
    "                                 lock=False)\n",
    "\n",
    "    def __contains__(self, key: K) -> bool:\n",
    "        if not key:\n",
    "            return False\n",
    "        with self.env.begin() as txn:\n",
    "            encoded_key = self.encode(key)\n",
    "            value = txn.get(encoded_key) if encoded_key else None\n",
    "        return value is not None\n",
    "\n",
    "    def close(self) -> None:\n",
    "        self.env.close()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "\n",
    "    def get(self, key: K, default_value=None) -> V:\n",
    "        with self.env.begin() as txn:\n",
    "            value = txn.get(self.encode(key))\n",
    "        if value is None:\n",
    "            return default_value\n",
    "        return self.decode(value)\n",
    "\n",
    "    def put(self, key: K, value: V):\n",
    "        if key is not None and value is not None:\n",
    "            with self.env.begin(write=True) as txn:\n",
    "                txn.put(key=key.encode(), value=json.dumps(value).encode())\n",
    "\n",
    "    def put_batch(self, keys: List[K], values: List[V]):\n",
    "        with self.env.begin(write=True) as txn:\n",
    "            for key, value in zip(keys, values):\n",
    "                try:\n",
    "                    txn.put(key=key.encode(), value=json.dumps(value).encode())\n",
    "                except lmdb.Error as err:\n",
    "                    logging.debug(f'skipping {key}, error: {err}')\n",
    "\n",
    "    def write_to_compacted_file(self):\n",
    "        \"\"\"\n",
    "        Writes memmap-based data structure to disk in compacted format\n",
    "        and deletes original over-allocated file.\n",
    "        Only call this method when this object is finished with.\n",
    "        \"\"\"\n",
    "        self.env.copy(path=self.path.replace('.incomplete', ''), compact=True)\n",
    "        os.remove(self.path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, input_dict: Dict[str, Any], output_file_path: str) -> 'LmdbImmutableDict[Any, Any]':\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f'Skipping conversion as {output_file_path} already exists.')\n",
    "        else:\n",
    "            output_lmdb_dict = LmdbImmutableDict(output_file_path,\n",
    "                                                 write_mode=True)\n",
    "            for batch in tqdm(list(batch_items(input_dict.items(), n=250000)),\n",
    "                              desc=f'Writing {output_file_path}'):\n",
    "                keys, values = zip(*batch)\n",
    "                output_lmdb_dict.put_batch(keys=keys, values=values)\n",
    "            output_lmdb_dict.write_to_compacted_file()\n",
    "        return cls(path=output_file_path, write_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b0ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qcode_to_wiki = LmdbImmutableDict(path=\"/home/morg/dataset/refined/organised_data_dir/wikidata_data/qcode_to_wiki.lmdb\", write_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577505ed",
   "metadata": {},
   "source": [
    "# Sample docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93de4f0-5095-4dca-a4f5-535207d92c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7696it [00:03, 2016.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu_id = 6\n",
    "output_path = f\"/home/morg/dataset/maverick/maverick_{gpu_id}.json\"\n",
    "\n",
    "long_documents = []\n",
    "short_documents = []\n",
    "max_chars = 50000\n",
    "start_index = 5114886\n",
    "\n",
    "def stream_ndjson(file_path, start):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            i = line.get(\"line_index\")\n",
    "            if i >= start:\n",
    "                yield (i, line)\n",
    "\n",
    "for line_index, doc in tqdm(stream_ndjson(output_path, start_index)):\n",
    "    if len(doc[\"text\"]) >= max_chars and len(long_documents) < 5:\n",
    "        long_documents.append(doc)\n",
    "    elif len(doc[\"text\"]) < max_chars and len(short_documents) < 5:\n",
    "        short_documents.append(doc)\n",
    "    if len(short_documents) == 5 and len(long_documents) == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db60ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = long_documents[0]\n",
    "hyperlinks = doc[\"hyperlinks_clean\"]\n",
    "coref = doc[\"coref\"]\n",
    "entity_linking = doc[\"entity_linking\"]\n",
    "title = doc[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b3ff5",
   "metadata": {},
   "source": [
    "# Merge metadata into clusters and score dominant entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ce46cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def is_pronoun(text):\n",
    "    return re.fullmatch(r\"\\b(?:he|she|it|they|we|i|you|him|her|us|them|me|my|your|his|their|our|its|mine|yours|hers|ours|theirs)\\b\", text.lower()) is not None\n",
    "\n",
    "def enrich_coref_clusters(coref, entity_linking, hyperlinks):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    enriched_clusters = defaultdict(list)\n",
    "\n",
    "    for cluster_id, cluster in enumerate(coref[\"clusters_char_offsets\"]):\n",
    "        for i, span in enumerate(cluster):\n",
    "            span_start, span_end = span\n",
    "            span_text = coref[\"clusters_char_text\"][cluster_id][i]\n",
    "            span_length = span_end - span_start\n",
    "\n",
    "            span_entry = {\n",
    "                \"start\": span_start,\n",
    "                \"end\": span_end,\n",
    "                \"coref_text\": span_text,\n",
    "                \"entities\": [],\n",
    "                \"links\": [],\n",
    "            }\n",
    "\n",
    "            for mention in entity_linking:\n",
    "                mention_start = mention[\"start\"]\n",
    "                mention_end = mention_start + mention[\"ln\"]\n",
    "\n",
    "                if not (mention_end <= span_start or mention_start >= span_end):\n",
    "                    entity_id = mention.get(\"predicted_entity\", {}).get(\"wikidata_entity_id\")\n",
    "                    if entity_id:\n",
    "                        overlap_start = max(span_start, mention_start)\n",
    "                        overlap_end = min(span_end, mention_end)\n",
    "                        overlap_length = overlap_end - overlap_start\n",
    "                        coverage_ratio = overlap_length / span_length if span_length > 0 else 0.0\n",
    "\n",
    "                        span_entry[\"entities\"].append({\n",
    "                            \"id\": entity_id,\n",
    "                            \"el_text\": mention[\"text\"],\n",
    "                            \"start\": mention_start,\n",
    "                            \"end\": mention_end,\n",
    "                            \"coverage_ratio\": coverage_ratio,\n",
    "                            \"exact\": mention_end == span_end and mention_start == span_start\n",
    "                        })\n",
    "\n",
    "            for link in hyperlinks:\n",
    "                link_start = link[\"start\"]\n",
    "                link_end = link[\"end\"]\n",
    "\n",
    "                if not (link_end <= span_start or link_start >= span_end):\n",
    "                    entity_id = link.get(\"qcode\")\n",
    "                    if entity_id:\n",
    "                        overlap_start = max(span_start, link_start)\n",
    "                        overlap_end = min(span_end, link_end)\n",
    "                        overlap_length = overlap_end - overlap_start\n",
    "                        coverage_ratio = overlap_length / span_length if span_length > 0 else 0.0\n",
    "\n",
    "                        span_entry[\"links\"].append({\n",
    "                            \"id\": entity_id,\n",
    "                            \"surface_form\": link[\"surface_form\"],\n",
    "                            \"start\": link_start,\n",
    "                            \"end\": link_end,\n",
    "                            \"coverage_ratio\": coverage_ratio,\n",
    "                            \"exact\": link_start == span_start and link_end == span_end\n",
    "                        })\n",
    "\n",
    "            enriched_clusters[cluster_id].append(span_entry)\n",
    "\n",
    "    return enriched_clusters\n",
    "\n",
    "\n",
    "def longest_common_substring(s1, s2):\n",
    "    s1 = s1.lower()\n",
    "    s2 = s2.lower()\n",
    "    m = len(s1)\n",
    "    n = len(s2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    longest = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if s1[i] == s2[j]:\n",
    "                dp[i + 1][j + 1] = dp[i][j] + 1\n",
    "                longest = max(longest, dp[i + 1][j + 1])\n",
    "\n",
    "    return longest\n",
    "\n",
    "def lcs_overlap_ratio(unlinked_text, anchor_text):\n",
    "    lcs_len = longest_common_substring(unlinked_text, anchor_text)\n",
    "    return lcs_len / len(anchor_text) if unlinked_text else 0.0\n",
    "\n",
    "def best_entity_or_link_match(span_text, anchor_spans):\n",
    "    span_text = span_text.lower()\n",
    "    best = {\n",
    "        \"match_type\": None,\n",
    "        \"anchor\": None,\n",
    "        \"item\": None,\n",
    "        \"similarity\": 0.0,\n",
    "    }\n",
    "\n",
    "    for anchor in anchor_spans:\n",
    "        for link in anchor.get(\"links\", []):\n",
    "            sim = lcs_overlap_ratio(span_text, link[\"surface_form\"])\n",
    "            if sim > best[\"similarity\"] or (sim == best[\"similarity\"] and best[\"match_type\"] != \"link\"):\n",
    "                best = {\n",
    "                    \"match_type\": \"link\",\n",
    "                    \"anchor\": anchor,\n",
    "                    \"item\": link,\n",
    "                    \"similarity\": sim,\n",
    "                }\n",
    "\n",
    "        for entity in anchor.get(\"entities\", []):\n",
    "            sim = lcs_overlap_ratio(span_text, entity[\"el_text\"])\n",
    "            if sim > best[\"similarity\"] or (sim == best[\"similarity\"] and best[\"match_type\"] is None):\n",
    "                best = {\n",
    "                    \"match_type\": \"entity\",\n",
    "                    \"anchor\": anchor,\n",
    "                    \"item\": entity,\n",
    "                    \"similarity\": sim,\n",
    "                }\n",
    "\n",
    "    return best if best[\"similarity\"] > 0.3 else None\n",
    "\n",
    "def score_entities_by_subject_likelihood(enriched_clusters):\n",
    "    cluster_entity_scores = {}\n",
    "\n",
    "    for cluster_id, spans in enriched_clusters.items():\n",
    "        entity_counts = defaultdict(float)\n",
    "        total_contribution = 0.0\n",
    "        seen_mentions = set()\n",
    "\n",
    "        anchor_spans = []\n",
    "        unlinked_spans = []\n",
    "\n",
    "        for span in spans:\n",
    "            if is_pronoun(span[\"coref_text\"]):\n",
    "                continue\n",
    "\n",
    "            has_entities = bool(span.get(\"entities\"))\n",
    "            has_links = bool(span.get(\"links\"))\n",
    "\n",
    "            if has_entities or has_links:\n",
    "                anchor_spans.append(span)\n",
    "\n",
    "                for entity in span.get(\"entities\", []):\n",
    "                    key = (entity[\"id\"], entity[\"start\"], entity[\"end\"])\n",
    "                    if key in seen_mentions:\n",
    "                        continue\n",
    "                    seen_mentions.add(key)\n",
    "\n",
    "                    weight = 0.85 * entity[\"coverage_ratio\"]\n",
    "                    entity_counts[entity[\"id\"]] += weight\n",
    "                    total_contribution += weight\n",
    "\n",
    "                for link in span.get(\"links\", []):\n",
    "                    key = (link[\"id\"], link[\"start\"], link[\"end\"])\n",
    "                    if key in seen_mentions:\n",
    "                        continue\n",
    "                    seen_mentions.add(key)\n",
    "\n",
    "                    weight = 1.0 * link[\"coverage_ratio\"]\n",
    "                    entity_counts[link[\"id\"]] += weight\n",
    "                    total_contribution += weight\n",
    "            else:\n",
    "                unlinked_spans.append(span)\n",
    "\n",
    "        # Infer contribution for unlinked spans via char-level overlap\n",
    "        for span in unlinked_spans:\n",
    "            result = best_entity_or_link_match(span[\"coref_text\"], anchor_spans)\n",
    "\n",
    "            if result:\n",
    "                sim = result[\"similarity\"]\n",
    "                if result[\"match_type\"] == \"entity\":\n",
    "                    entity = result[\"item\"]\n",
    "                    weight = sim * 0.85 * entity.get(\"coverage_ratio\", 0.5)\n",
    "                    entity_counts[entity[\"id\"]] += weight\n",
    "                    total_contribution += weight\n",
    "\n",
    "                elif result[\"match_type\"] == \"link\":\n",
    "                    link = result[\"item\"]\n",
    "                    weight = sim * 1.0 * link.get(\"coverage_ratio\", 0.5)\n",
    "                    entity_counts[link[\"id\"]] += weight\n",
    "                    total_contribution += weight\n",
    "\n",
    "        # print(total_contribution)\n",
    "        total_contribution = max(1.0, total_contribution)\n",
    "\n",
    "        normalized_scores = {\n",
    "            entity_id: count / total_contribution\n",
    "            for entity_id, count in entity_counts.items()\n",
    "        } if total_contribution > 0 else {}\n",
    "\n",
    "        cluster_entity_scores[cluster_id] = normalized_scores\n",
    "\n",
    "    return cluster_entity_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a20c7",
   "metadata": {},
   "source": [
    "# Display clusters + entities in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20d7f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def analyze_coref_clusters_interactive_with_merge(doc):\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "\n",
    "    text = doc[\"text\"]\n",
    "    coref = doc[\"coref\"]\n",
    "    entity_linking = doc[\"entity_linking\"]\n",
    "    hyperlinks = doc[\"hyperlinks_clean\"]\n",
    "\n",
    "    enriched_clusters = enrich_coref_clusters(coref, entity_linking, hyperlinks)\n",
    "\n",
    "    # Keep only clusters that contain at least one span with an entity or link\n",
    "    filtered_enriched_clusters = {\n",
    "        cid: spans for cid, spans in enriched_clusters.items()\n",
    "        if any(span.get(\"entities\") or span.get(\"links\") for span in spans)\n",
    "    }\n",
    "    enriched_clusters = filtered_enriched_clusters\n",
    "\n",
    "    cluster_ids = sorted(enriched_clusters.keys())\n",
    "    entity_scores = score_entities_by_subject_likelihood(enriched_clusters)\n",
    "\n",
    "    def cluster_similarity_score(cluster_a_id, cluster_b_id, enriched_clusters):\n",
    "        spans_a = enriched_clusters[cluster_a_id]\n",
    "        spans_b = enriched_clusters[cluster_b_id]\n",
    "\n",
    "        text_to_entities_a = defaultdict(Counter)\n",
    "        text_to_entities_b = defaultdict(Counter)\n",
    "\n",
    "        for span in spans_a:\n",
    "            if not is_pronoun(span[\"coref_text\"]):\n",
    "                text_key = span[\"coref_text\"].strip().lower()\n",
    "                qids = [ent[\"id\"] for ent in span[\"entities\"]] + [link[\"id\"] for link in span[\"links\"]]\n",
    "                if qids:\n",
    "                    text_to_entities_a[text_key].update(qids)\n",
    "                else:\n",
    "                    text_to_entities_a[text_key][\"<NONE>\"] += 1\n",
    "\n",
    "        for span in spans_b:\n",
    "            if not is_pronoun(span[\"coref_text\"]):\n",
    "                text_key = span[\"coref_text\"].strip().lower()\n",
    "                qids = [ent[\"id\"] for ent in span[\"entities\"]] + [link[\"id\"] for link in span[\"links\"]]\n",
    "                if qids:\n",
    "                    text_to_entities_b[text_key].update(qids)\n",
    "                else:\n",
    "                    text_to_entities_b[text_key][\"<NONE>\"] += 1\n",
    "\n",
    "        shared_text = set(text_to_entities_b.keys()).intersection(set(text_to_entities_a.keys()))\n",
    "\n",
    "        total_text_freq = Counter()\n",
    "        for text, counter in text_to_entities_a.items():\n",
    "            total_text_freq[text] += sum(counter.values())\n",
    "        for text, counter in text_to_entities_b.items():\n",
    "            total_text_freq[text] += sum(counter.values())\n",
    "\n",
    "        overlap_score = 0\n",
    "        for text in shared_text:\n",
    "            entities_a = text_to_entities_a[text]\n",
    "            entities_b = text_to_entities_b[text]\n",
    "\n",
    "            qids_a = set(entities_a.keys())\n",
    "            qids_b = set(entities_b.keys())\n",
    "\n",
    "            qids_a.discard(\"<NONE>\")\n",
    "            qids_b.discard(\"<NONE>\")\n",
    "\n",
    "            freq_a = sum(entities_a.values())\n",
    "            freq_b = sum(entities_b.values())\n",
    "\n",
    "            overlap_score += (freq_a + freq_b)\n",
    "\n",
    "        total = sum(total_text_freq.values())\n",
    "        return overlap_score / total if total > 0 else 0\n",
    "\n",
    "    def render_cluster_html(text, cluster_id, spans, scores, merged_info):\n",
    "        sorted_entities = sorted(scores.items(), key=lambda x: -x[1])\n",
    "\n",
    "        html_chunks = []\n",
    "\n",
    "        merged_str = \", \".join(\n",
    "            f\"{cid} ({score:.2f})\" if cid != cluster_id else f\"{cid} (base)\"\n",
    "            for cid, score in merged_info\n",
    "        )\n",
    "        html_chunks.append(f\"<h3>Cluster {cluster_id} (merged with: {merged_str})</h3>\")\n",
    "        html_chunks.append(\"<ul>\")\n",
    "        for entity_id, score in sorted_entities:\n",
    "            name = qcode_to_wiki.get(entity_id, \"Unknown\")\n",
    "            html_chunks.append(f\"<li><b>{entity_id}</b> ({name}): {score:.3f}</li>\")\n",
    "        html_chunks.append(\"</ul>\")\n",
    "\n",
    "        annotated_text = list(text)\n",
    "        tags_to_insert = []\n",
    "\n",
    "        for span in spans:\n",
    "            for entity in span.get(\"entities\", []):\n",
    "                tags_to_insert.append((entity[\"start\"], '<span style=\"background-color: #aaf;\">'))\n",
    "                tags_to_insert.append((entity[\"end\"], '</span>'))\n",
    "\n",
    "            for link in span.get(\"links\", []):\n",
    "                tags_to_insert.append((link[\"start\"], '<span style=\"background-color: #afa;\">'))\n",
    "                tags_to_insert.append((link[\"end\"], '</span>'))\n",
    "\n",
    "            tags_to_insert.append((span[\"start\"], '<span style=\"background-color: #aaf;\">'))\n",
    "            tags_to_insert.append((span[\"end\"], '</span>'))\n",
    "\n",
    "        tags_to_insert.sort(reverse=True)\n",
    "        for pos, tag in tags_to_insert:\n",
    "            annotated_text.insert(pos, tag)\n",
    "\n",
    "        annotated_string = \"\".join(annotated_text)\n",
    "        html_chunks.append(f\"<div style='border:1px solid #ccc; padding:10px; margin:10px 0;'>{annotated_string}</div>\")\n",
    "\n",
    "        return \"\\n\".join(html_chunks)\n",
    "\n",
    "    def update_view(cluster_id, threshold):\n",
    "        base_cluster = enriched_clusters[cluster_id]\n",
    "        merged_spans = list(base_cluster)\n",
    "        merged_info = [(cluster_id, 1.0)]\n",
    "\n",
    "        debug_cluster_similarity = []\n",
    "        for other_id in enriched_clusters:\n",
    "            if other_id == cluster_id:\n",
    "                continue\n",
    "            score = cluster_similarity_score(cluster_id, other_id, enriched_clusters)\n",
    "            if score > 0:\n",
    "                debug_cluster_similarity.append((other_id, score))\n",
    "            if score >= threshold:\n",
    "                merged_spans.extend(enriched_clusters[other_id])\n",
    "                merged_info.append((other_id, score))\n",
    "\n",
    "        debug_cluster_similarity = sorted(debug_cluster_similarity, key=lambda x: -x[1])\n",
    "        debug_cluster_similarity_str = \"\\n\".join(\n",
    "            f\"Overlap similarity with Cluster {cid}: ({score:.2f})\"\n",
    "            for cid, score in debug_cluster_similarity\n",
    "        )\n",
    "        print(debug_cluster_similarity_str)\n",
    "\n",
    "        seen = set()\n",
    "        unique_spans = []\n",
    "        for span in merged_spans:\n",
    "            key = (span[\"start\"], span[\"end\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_spans.append(span)\n",
    "        unique_spans.sort(key=lambda s: s[\"start\"])\n",
    "\n",
    "        temp_cluster = {0: unique_spans}\n",
    "        scores = score_entities_by_subject_likelihood(temp_cluster)\n",
    "        return render_cluster_html(text, cluster_id, unique_spans, scores[0], merged_info)\n",
    "\n",
    "    # Widgets\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=cluster_ids,\n",
    "        description='Cluster:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    threshold_slider = widgets.FloatSlider(\n",
    "        value=0.6,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.05,\n",
    "        description='Merge Threshold:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    search_box = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Type to search text or QID...',\n",
    "        description='Search:',\n",
    "        style={'description_width': 'initial'},\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    qid_score_threshold_slider = widgets.FloatSlider(\n",
    "        value=0.0,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.01,\n",
    "        description='QID Score Threshold:',\n",
    "        style={'description_width': 'initial'},\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    qid_score_filter_direction = widgets.Dropdown(\n",
    "        options=[('≥ (Above)', 'above'), ('≤ (Below)', 'below')],\n",
    "        value='above',\n",
    "        description='Filter Direction:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def find_relevant_clusters(query):\n",
    "        query = query.strip().lower()\n",
    "        if not query:\n",
    "            return cluster_ids\n",
    "\n",
    "        # QID search (e.g., Q42)\n",
    "        if query.upper().startswith(\"Q\") and query[1:].isdigit():\n",
    "            qid = query.upper()\n",
    "            direction = qid_score_filter_direction.value\n",
    "            threshold_val = qid_score_threshold_slider.value\n",
    "\n",
    "            scored_clusters = [\n",
    "                (cid, scores[qid])\n",
    "                for cid, scores in entity_scores.items()\n",
    "                if qid in scores\n",
    "            ]\n",
    "\n",
    "            if direction == 'above':\n",
    "                scored_clusters = [item for item in scored_clusters if item[1] >= threshold_val]\n",
    "            else:\n",
    "                scored_clusters = [item for item in scored_clusters if item[1] <= threshold_val]\n",
    "\n",
    "            if scored_clusters:\n",
    "                return [cid for cid, _ in sorted(scored_clusters, key=lambda x: -x[1])]\n",
    "\n",
    "        scored_clusters = []\n",
    "        fallback_clusters = []\n",
    "\n",
    "        for cid, spans in enriched_clusters.items():\n",
    "            if not any(query in span[\"coref_text\"].lower() for span in spans):\n",
    "                continue\n",
    "\n",
    "            scores = entity_scores.get(cid, {})\n",
    "            match_score = None\n",
    "            for qid, score in scores.items():\n",
    "                qid_name = qcode_to_wiki.get(qid, \"\").lower()\n",
    "                if qid_name == query:\n",
    "                    match_score = score\n",
    "                    break\n",
    "\n",
    "            if match_score is not None:\n",
    "                scored_clusters.append((cid, match_score))\n",
    "            else:\n",
    "                fallback_clusters.append(cid)\n",
    "\n",
    "        sorted_scored = [cid for cid, _ in sorted(scored_clusters, key=lambda x: (-x[1], x[0]))]\n",
    "        fallback_clusters.sort()\n",
    "        return sorted_scored + fallback_clusters\n",
    "\n",
    "    def update_output_with_search(*args):\n",
    "        with output:\n",
    "            query = search_box.value.strip()\n",
    "            relevant_ids = find_relevant_clusters(query)\n",
    "            threshold = threshold_slider.value\n",
    "\n",
    "            if not query:\n",
    "                dropdown.options = cluster_ids\n",
    "                dropdown.value = cluster_ids[0]\n",
    "            elif relevant_ids:\n",
    "                dropdown.options = relevant_ids\n",
    "                dropdown.value = relevant_ids[0]\n",
    "            else:\n",
    "                dropdown.options = cluster_ids\n",
    "                clear_output()\n",
    "                display(HTML(f\"<p>No clusters found for search '<b>{query}</b>'.</p>\"))\n",
    "                return\n",
    "\n",
    "            clear_output()\n",
    "            html = update_view(dropdown.value, threshold)\n",
    "            display(HTML(html))\n",
    "\n",
    "    def update_output_with_dropdown(change):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            cluster_id = dropdown.value\n",
    "            threshold = threshold_slider.value\n",
    "            html = update_view(cluster_id, threshold)\n",
    "            display(HTML(html))\n",
    "\n",
    "    # Bind widget listeners\n",
    "    dropdown.observe(update_output_with_dropdown, names='value')\n",
    "    threshold_slider.observe(update_output_with_search, names='value')\n",
    "    search_box.observe(update_output_with_search, names='value')\n",
    "    qid_score_threshold_slider.observe(update_output_with_search, names='value')\n",
    "    qid_score_filter_direction.observe(update_output_with_search, names='value')\n",
    "\n",
    "    # Initial display\n",
    "    display(widgets.VBox([\n",
    "        search_box,\n",
    "        widgets.HBox([dropdown, threshold_slider]),\n",
    "        widgets.HBox([qid_score_threshold_slider, qid_score_filter_direction]),\n",
    "        output\n",
    "    ]))\n",
    "    update_output_with_search()\n",
    "\n",
    "    return enriched_clusters, entity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cdc32ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adc1b306d30407cb3475f6b1da94e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', continuous_update=False, description='Search:', placeholder='Type to search text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enriched_clusters, entity_scores = analyze_coref_clusters_interactive_with_merge(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9028c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8468518518518519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{227: {'Q27980167': 0.595,\n",
       "  'Q3894681': 0.07345679012345678,\n",
       "  'Q1196645': 0.0839506172839506,\n",
       "  'Q3065717': 0.09444444444444444}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_entities_by_subject_likelihood({227: enriched_clusters[227]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f51a3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def analyze_coref_clusters_interactive_with_merge(doc):\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "\n",
    "    text = doc[\"text\"]\n",
    "    coref = doc[\"coref\"]\n",
    "    entity_linking = doc[\"entity_linking\"]\n",
    "    hyperlinks = doc[\"hyperlinks_clean\"]\n",
    "\n",
    "    enriched_clusters = enrich_coref_clusters(coref, entity_linking, hyperlinks)\n",
    "\n",
    "    # Keep only clusters that contain at least one span with an entity or link\n",
    "    enriched_clusters = {\n",
    "        cid: spans for cid, spans in enriched_clusters.items()\n",
    "        if any(span.get(\"entities\") or span.get(\"links\") for span in spans)\n",
    "    }\n",
    "\n",
    "    cluster_ids = sorted(enriched_clusters.keys())\n",
    "    entity_scores = score_entities_by_subject_likelihood(enriched_clusters)\n",
    "\n",
    "    def cluster_similarity_score(cluster_a_id, cluster_b_id, enriched_clusters):\n",
    "        spans_a = enriched_clusters[cluster_a_id]\n",
    "        spans_b = enriched_clusters[cluster_b_id]\n",
    "\n",
    "        text_to_entities_a = defaultdict(Counter)\n",
    "        text_to_entities_b = defaultdict(Counter)\n",
    "\n",
    "        for span in spans_a:\n",
    "            if not is_pronoun(span[\"coref_text\"]):\n",
    "                text_key = span[\"coref_text\"].strip().lower()\n",
    "                qids = [ent[\"id\"] for ent in span[\"entities\"]] + [link[\"id\"] for link in span[\"links\"]]\n",
    "                if qids:\n",
    "                    text_to_entities_a[text_key].update(qids)\n",
    "                else:\n",
    "                    text_to_entities_a[text_key][\"<NONE>\"] += 1\n",
    "\n",
    "        for span in spans_b:\n",
    "            if not is_pronoun(span[\"coref_text\"]):\n",
    "                text_key = span[\"coref_text\"].strip().lower()\n",
    "                qids = [ent[\"id\"] for ent in span[\"entities\"]] + [link[\"id\"] for link in span[\"links\"]]\n",
    "                if qids:\n",
    "                    text_to_entities_b[text_key].update(qids)\n",
    "                else:\n",
    "                    text_to_entities_b[text_key][\"<NONE>\"] += 1\n",
    "\n",
    "        shared_text = set(text_to_entities_b.keys()).intersection(set(text_to_entities_a.keys()))\n",
    "\n",
    "        total_text_freq = Counter()\n",
    "        for text, counter in text_to_entities_a.items():\n",
    "            total_text_freq[text] += sum(counter.values())\n",
    "        for text, counter in text_to_entities_b.items():\n",
    "            total_text_freq[text] += sum(counter.values())\n",
    "\n",
    "        overlap_score = 0\n",
    "        for text in shared_text:\n",
    "            entities_a = text_to_entities_a[text]\n",
    "            entities_b = text_to_entities_b[text]\n",
    "\n",
    "            qids_a = set(entities_a.keys())\n",
    "            qids_b = set(entities_b.keys())\n",
    "\n",
    "            qids_a.discard(\"<NONE>\")\n",
    "            qids_b.discard(\"<NONE>\")\n",
    "\n",
    "            freq_a = sum(entities_a.values())\n",
    "            freq_b = sum(entities_b.values())\n",
    "\n",
    "            overlap_score += (freq_a + freq_b)\n",
    "\n",
    "        total = sum(total_text_freq.values())\n",
    "        return overlap_score / total if total > 0 else 0\n",
    "\n",
    "    def render_cluster_html(text, cluster_id, spans, scores, merged_info):\n",
    "        sorted_entities = sorted(scores.items(), key=lambda x: -x[1])\n",
    "\n",
    "        html_chunks = []\n",
    "\n",
    "        merged_str = \", \".join(\n",
    "            f\"{cid} ({score:.2f})\" if cid != cluster_id else f\"{cid} (base)\"\n",
    "            for cid, score in merged_info\n",
    "        )\n",
    "        html_chunks.append(f\"<h3>Cluster {cluster_id} (merged with: {merged_str})</h3>\")\n",
    "        html_chunks.append(\"<ul>\")\n",
    "        for entity_id, score in sorted_entities:\n",
    "            name = qcode_to_wiki.get(entity_id, \"Unknown\")\n",
    "            html_chunks.append(f\"<li><b>{entity_id}</b> ({name}): {score:.3f}</li>\")\n",
    "        html_chunks.append(\"</ul>\")\n",
    "\n",
    "        annotated_text = list(text)\n",
    "        tags_to_insert = []\n",
    "\n",
    "        for span in spans:\n",
    "            for entity in span.get(\"entities\", []):\n",
    "                tags_to_insert.append((entity[\"start\"], '<span style=\"background-color: #aaf;\">'))\n",
    "                tags_to_insert.append((entity[\"end\"], '</span>'))\n",
    "\n",
    "            for link in span.get(\"links\", []):\n",
    "                tags_to_insert.append((link[\"start\"], '<span style=\"background-color: #afa;\">'))\n",
    "                tags_to_insert.append((link[\"end\"], '</span>'))\n",
    "\n",
    "            tags_to_insert.append((span[\"start\"], '<span style=\"background-color: #aaf;\">'))\n",
    "            tags_to_insert.append((span[\"end\"], '</span>'))\n",
    "\n",
    "        tags_to_insert.sort(reverse=True)\n",
    "        for pos, tag in tags_to_insert:\n",
    "            annotated_text.insert(pos, tag)\n",
    "\n",
    "        annotated_string = \"\".join(annotated_text)\n",
    "        html_chunks.append(f\"<div style='border:1px solid #ccc; padding:10px; margin:10px 0;'>{annotated_string}</div>\")\n",
    "\n",
    "        return \"\\n\".join(html_chunks)\n",
    "\n",
    "    def update_view(cluster_id, threshold):\n",
    "        base_cluster = enriched_clusters[cluster_id]\n",
    "        merged_spans = list(base_cluster)\n",
    "        merged_info = [(cluster_id, 1.0)]\n",
    "\n",
    "        debug_cluster_similarity = []\n",
    "        for other_id in enriched_clusters:\n",
    "            if other_id == cluster_id:\n",
    "                continue\n",
    "            score = cluster_similarity_score(cluster_id, other_id, enriched_clusters)\n",
    "            if score > 0:\n",
    "                debug_cluster_similarity.append((other_id, score))\n",
    "            if score >= threshold:\n",
    "                merged_spans.extend(enriched_clusters[other_id])\n",
    "                merged_info.append((other_id, score))\n",
    "\n",
    "        debug_cluster_similarity = sorted(debug_cluster_similarity, key=lambda x: -x[1])\n",
    "\n",
    "        seen = set()\n",
    "        unique_spans = []\n",
    "        for span in merged_spans:\n",
    "            key = (span[\"start\"], span[\"end\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                unique_spans.append(span)\n",
    "        unique_spans.sort(key=lambda s: s[\"start\"])\n",
    "\n",
    "        temp_cluster = {0: unique_spans}\n",
    "        scores = score_entities_by_subject_likelihood(temp_cluster)\n",
    "        return render_cluster_html(text, cluster_id, unique_spans, scores[0], merged_info)\n",
    "\n",
    "    # Widgets\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=cluster_ids,\n",
    "        description='Cluster:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    threshold_slider = widgets.FloatSlider(\n",
    "        value=0.6,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.05,\n",
    "        description='Merge Threshold:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    search_box = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Type to search text or QID...',\n",
    "        description='Search:',\n",
    "        style={'description_width': 'initial'},\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    qid_score_threshold_slider = widgets.FloatSlider(\n",
    "        value=0.0,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.01,\n",
    "        description='QID Score Threshold:',\n",
    "        style={'description_width': 'initial'},\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    qid_score_filter_direction = widgets.Dropdown(\n",
    "        options=[('≥ (Above)', 'above'), ('≤ (Below)', 'below')],\n",
    "        value='above',\n",
    "        description='Filter Direction:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    view_all_qid_mentions_toggle = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Show all QID mentions',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def find_relevant_clusters(query):\n",
    "        query = query.strip().lower()\n",
    "        if not query:\n",
    "            return cluster_ids\n",
    "\n",
    "        if query.upper().startswith(\"Q\") and query[1:].isdigit():\n",
    "            qid = query.upper()\n",
    "            direction = qid_score_filter_direction.value\n",
    "            threshold_val = qid_score_threshold_slider.value\n",
    "\n",
    "            scored_clusters = [\n",
    "                (cid, scores[qid])\n",
    "                for cid, scores in entity_scores.items()\n",
    "                if qid in scores\n",
    "            ]\n",
    "\n",
    "            if direction == 'above':\n",
    "                scored_clusters = [item for item in scored_clusters if item[1] >= threshold_val]\n",
    "            else:\n",
    "                scored_clusters = [item for item in scored_clusters if item[1] <= threshold_val]\n",
    "\n",
    "            if scored_clusters:\n",
    "                return [cid for cid, _ in sorted(scored_clusters, key=lambda x: -x[1])]\n",
    "\n",
    "        scored_clusters = []\n",
    "        fallback_clusters = []\n",
    "\n",
    "        for cid, spans in enriched_clusters.items():\n",
    "            if not any(query in span[\"coref_text\"].lower() for span in spans):\n",
    "                continue\n",
    "\n",
    "            scores = entity_scores.get(cid, {})\n",
    "            match_score = None\n",
    "            for qid, score in scores.items():\n",
    "                qid_name = qcode_to_wiki.get(qid, \"\").lower()\n",
    "                if qid_name == query:\n",
    "                    match_score = score\n",
    "                    break\n",
    "\n",
    "            if match_score is not None:\n",
    "                scored_clusters.append((cid, match_score))\n",
    "            else:\n",
    "                fallback_clusters.append(cid)\n",
    "\n",
    "        sorted_scored = [cid for cid, _ in sorted(scored_clusters, key=lambda x: (-x[1], x[0]))]\n",
    "        fallback_clusters.sort()\n",
    "        return sorted_scored + fallback_clusters\n",
    "\n",
    "    def update_output_with_search(*args):\n",
    "        with output:\n",
    "            query = search_box.value.strip()\n",
    "            threshold = threshold_slider.value\n",
    "            view_all = view_all_qid_mentions_toggle.value\n",
    "\n",
    "            if query.upper().startswith(\"Q\") and query[1:].isdigit() and view_all:\n",
    "                qid = query.upper()\n",
    "                direction = qid_score_filter_direction.value\n",
    "                threshold_val = qid_score_threshold_slider.value\n",
    "\n",
    "                scored_clusters = [\n",
    "                    (cid, scores[qid])\n",
    "                    for cid, scores in entity_scores.items()\n",
    "                    if qid in scores\n",
    "                ]\n",
    "\n",
    "                if direction == 'above':\n",
    "                    scored_clusters = [item for item in scored_clusters if item[1] >= threshold_val]\n",
    "                else:\n",
    "                    scored_clusters = [item for item in scored_clusters if item[1] <= threshold_val]\n",
    "\n",
    "                if not scored_clusters:\n",
    "                    dropdown.options = cluster_ids\n",
    "                    clear_output()\n",
    "                    display(HTML(f\"<p>No clusters found for QID '<b>{qid}</b>'.</p>\"))\n",
    "                    return\n",
    "\n",
    "                # Aggregate spans\n",
    "                spans = []\n",
    "                merged_info = []\n",
    "                for cid, score in scored_clusters:\n",
    "                    spans.extend(enriched_clusters[cid])\n",
    "                    merged_info.append((cid, score))\n",
    "\n",
    "                # Deduplicate\n",
    "                seen = set()\n",
    "                unique_spans = []\n",
    "                for span in spans:\n",
    "                    key = (span[\"start\"], span[\"end\"])\n",
    "                    if key not in seen:\n",
    "                        seen.add(key)\n",
    "                        unique_spans.append(span)\n",
    "                unique_spans.sort(key=lambda s: s[\"start\"])\n",
    "\n",
    "                temp_cluster = {0: unique_spans}\n",
    "                scores = score_entities_by_subject_likelihood(temp_cluster)\n",
    "                html = render_cluster_html(text, qid, unique_spans, scores[0], merged_info)\n",
    "\n",
    "                clear_output()\n",
    "                display(HTML(html))\n",
    "                return\n",
    "\n",
    "            relevant_ids = find_relevant_clusters(query)\n",
    "            if not query:\n",
    "                dropdown.options = cluster_ids\n",
    "                dropdown.value = cluster_ids[0]\n",
    "            elif relevant_ids:\n",
    "                dropdown.options = relevant_ids\n",
    "                dropdown.value = relevant_ids[0]\n",
    "            else:\n",
    "                dropdown.options = cluster_ids\n",
    "                clear_output()\n",
    "                display(HTML(f\"<p>No clusters found for search '<b>{query}</b>'.</p>\"))\n",
    "                return\n",
    "\n",
    "            clear_output()\n",
    "            html = update_view(dropdown.value, threshold)\n",
    "            display(HTML(html))\n",
    "\n",
    "    def update_output_with_dropdown(change):\n",
    "        with output:\n",
    "            cluster_id = dropdown.value\n",
    "            threshold = threshold_slider.value\n",
    "            html = update_view(cluster_id, threshold)\n",
    "            clear_output()\n",
    "            display(HTML(html))\n",
    "\n",
    "    # Bind widget listeners\n",
    "    dropdown.observe(update_output_with_dropdown, names='value')\n",
    "    threshold_slider.observe(update_output_with_search, names='value')\n",
    "    search_box.observe(update_output_with_search, names='value')\n",
    "    qid_score_threshold_slider.observe(update_output_with_search, names='value')\n",
    "    qid_score_filter_direction.observe(update_output_with_search, names='value')\n",
    "    view_all_qid_mentions_toggle.observe(update_output_with_search, names='value')\n",
    "\n",
    "    # Initial display\n",
    "    display(widgets.VBox([\n",
    "        search_box,\n",
    "        widgets.HBox([dropdown, threshold_slider]),\n",
    "        widgets.HBox([qid_score_threshold_slider, qid_score_filter_direction]),\n",
    "        view_all_qid_mentions_toggle,\n",
    "        output\n",
    "    ]))\n",
    "    update_output_with_search()\n",
    "\n",
    "    return enriched_clusters, entity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ae3e605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5e0e42fd064a61af47d16b174331e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', continuous_update=False, description='Search:', placeholder='Type to search text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enriched_clusters, entity_scores = analyze_coref_clusters_interactive_with_merge(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b67fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
